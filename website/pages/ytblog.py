import googleapiclient.discovery
import requests
from bs4 import BeautifulSoup

YOUTUBE_API_KEY = 'AIzaSyDzQngwEKSXznBCvRoURIXs4-WZeps4uHA'
CHANNEL_ID = 'UC-YZGy4Uf_X8UjTZvg1tCCQ'

def get_latest_videos(channel_id, max_results=10):
    youtube = googleapiclient.discovery.build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
    request = youtube.search().list(part="snippet", channelId=channel_id, maxResults=max_results, order="viewCount", type="video")
    response = request.execute()

    videos = []
    for item in response['items']:
        video_id = item['id']['videoId']
        title = item['snippet']['title']
        videos.append((video_id, title))
    
    return videos

from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound

def get_english_transcript(video_id):
    # Try fetching the manually created transcript first
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
        return ' '.join([entry['text'] for entry in transcript])
    except (TranscriptsDisabled, NoTranscriptFound):
        pass

    # If the above fails, try fetching the auto-generated transcript
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'], remove_autogenerated=False)
        return ' '.join([entry['text'] for entry in transcript])
    except Exception as e:
        print(f"Error fetching transcript for video {video_id}: {e}")
        return ""

def count_words(text):
    return len(text.split())

videos = get_latest_videos(CHANNEL_ID)

for video_id, title in videos:
    print(f"Fetching transcript for video: {title}")
    transcript = get_english_transcript(video_id)

    if count_words(transcript) > 3000:
        print(f"Transcript for video {title} is too long. Skipping...\n")
        continue

    print(f"Transcript for video {title}:\n{transcript[:50000]}...")  # Printing only the first 300 characters for brevity.

    


import googleapiclient.discovery
import os
import django
import random
import requests
from bs4 import BeautifulSoup
import sys
sys.path.append('/home/rohit/news/website')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'website.settings')
django.setup()

from PIL import Image
import openai

from google_images_search import GoogleImagesSearch
import itertools

import shutil
import tempfile


import re

from pages.models import GeneratedBlog

openai.api_key = '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'
YOUTUBE_API_KEY = 'AIzaSyDzQngwEKSXznBCvRoURIXs4-WZeps4uHA'
CHANNEL_ID = 'UC-YZGy4Uf_X8UjTZvg1tCCQ'

def get_latest_videos(channel_id, max_results=10):
    youtube = googleapiclient.discovery.build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
    request = youtube.search().list(part="snippet", channelId=channel_id, maxResults=max_results, order="viewCount", type="video")
    response = request.execute()

    videos = []
    for item in response['items']:
        video_id = item['id']['videoId']
        title = item['snippet']['title']
        videos.append((video_id, title))
    
    return videos

from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound


def get_english_transcript(video_id):
    # Try fetching the manually created transcript first
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
        return ' '.join([entry['text'] for entry in transcript])
    except (TranscriptsDisabled, NoTranscriptFound):
        pass

    # If the above fails, try fetching the auto-generated transcript
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'], remove_autogenerated=False)
        return ' '.join([entry['text'] for entry in transcript])
    except Exception as e:
        print(f"Error fetching transcript for video {video_id}: {e}")
        return ""


def count_words(text):
    return len(text.split())



def generate_image_search_query(blog_title):
    model = "gpt-3.5-turbo"

    prompt = f'''
    I have written a blog titled '{blog_title}'. Can you suggest a Google search query to find a suitable image for this blog?
    '''

    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a skilled blogger and SEO expert."},
            {"role": "user", "content": prompt},
        ],
        max_tokens=60,
        temperature=0.7
    )

    response_text = response.choices[0].message['content']

    # Extract the text between single quotes
    search_query = re.findall(r'[\'"](.*?)[\'"]', response_text)

    if search_query:
        # If there's a match, return the first group found
        suggested_query = search_query[0]
    else:
        # If no match is found, return the whole response
        suggested_query = response_text

    print(suggested_query)
    return suggested_query



def get_google_image(blog_title):
    # Provide your Google Custom Search API keys and CXs
    query = generate_image_search_query(blog_title)

    api_keys = [
        ('AIzaSyASfWANIMbq1Ta9y8x5DHIbPwOyOawHCsc', 'f4d874a23b5564000'),
        ('AIzaSyCyziO3H25aTxAGzPfLk34q1_gVL6vW62M', '3703ccef476244145'),
        ('AIzaSyCj03npWHxB9EN64MqhiuRTOHMQyIWK7DU', 'f2da21573b2f1485b'),
    ]

    for api_key, cx in api_keys:
        try:
            # Create a new GoogleImagesSearch object with the current API key and CX
            gis = GoogleImagesSearch(api_key, cx)

            truncated_query = ' '.join(query.split()[:5])

            _search_params = {
                'q': query,   # Search query
                'num': 10,    # Number of results to return
                'fileType': 'jpg|png',  # File types to return
            }

            # Search for images
            gis.search(search_params=_search_params)

            # Check if any results were returned
            if gis.results():
                for image in gis.results():
                    # Create a temporary directory and download the image there
                    with tempfile.TemporaryDirectory() as temp_dir:
                        image.download(temp_dir)
                        downloaded_image_path = os.path.join(temp_dir, os.listdir(temp_dir)[0])  # Get the path of the downloaded image

                        # Open the image using PIL
                        img = Image.open(downloaded_image_path)

                        # Check the image's width
                        if img.width >= 800:
                            # Define the image keyword used in the file name
                            safe_image_keywords = ' '.join(re.sub(r'[^\w\s]', '', truncated_query).split()[-4:])

                            # Define paths for the .jpg and .webp files
                            jpg_file_name = f"{safe_image_keywords}.jpg"
                            webp_file_name = f"{safe_image_keywords}.webp"

                            # Define the final paths for the images
                            base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # Get the Django base directory
                            jpg_file_path = os.path.join(base_dir, 'media', jpg_file_name)
                            webp_file_path = os.path.join(base_dir, 'media', webp_file_name)

                            # Ensure the directory exists
                            os.makedirs(os.path.dirname(jpg_file_path), exist_ok=True)

                            # Move the downloaded image to the desired location with the desired name
                            shutil.move(downloaded_image_path, jpg_file_path)

                            # Convert the image to webp
                            img = Image.open(jpg_file_path)
                            img = img.convert('RGB')
                            img.save(webp_file_path, 'webp', quality=20)

                            # Remove the original jpg file
                            os.remove(jpg_file_path)

                            return webp_file_path

            # If no results were returned or none of the images were large enough, continue to the next API key
            print("No suitable images found. Switching to the next API key...")
            continue

        except Exception as e:
            # If there is an API error (quota reached or other error), switch to the next API key and continue to the next iteration
            print(f"Exception: {e}")
            print("Switching to the next API key...")
            continue

    # If all API keys have reached the quota or there was an issue with all keys, return the default image
    print("All API keys have reached the quota or there was an issue with all keys.")
    return '/default.webp'  # Return default image if no images are found

# ... (previous code)




def generate_blog_post(video_title, video_transcript):
    model = "gpt-3.5-turbo"
    prompt = f'''
    Dear AI, I came across a video titled "{video_title}" with the following content: "{video_transcript}". It really caught my attention, and I think it would make an excellent topic for a short blog post. Could you help me with this? Please craft a compelling piece based on the video content. Thanks!
    '''

    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a news blogger."},
            {"role": "user", "content": prompt},
        ],
        max_tokens=700,
        temperature=random.uniform(0.3, 0.7)
    )

    blog_post = response.choices[0].message['content']

    # Assuming the first line is the title
    title = blog_post.split('\n', 1)[0].strip()
    content = blog_post.split('\n', 1)[1].strip() if len(blog_post.split('\n', 1)) > 1 else ""

    # Here, further processing can be done based on your requirements

    return title, content

videos = get_latest_videos(CHANNEL_ID)

for video_id, title in videos:
    print(f"Fetching transcript for video: {title}")
    transcript = get_english_transcript(video_id)

    if count_words(transcript) > 3000:
        print(f"Transcript for video {title} is too long. Skipping...\n")
        continue

    print(f"Transcript for video {title}:\n{transcript[:500]}...")  # Printing only the first 500 characters for brevity.

    blog_title, blog_content = generate_blog_post(title, transcript)

    # Get image for the blog based on the title
    blog_image = get_google_image(blog_title)

    # Save to Django model
    blog = GeneratedBlog(
        title=blog_title,
        content=blog_content,
        image=blog_image,
        # other fields can be added as necessary
    )
    blog.save()

    print(f'Saved the blog titled: {blog_title}')